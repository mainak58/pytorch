{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ccfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)  # [B, C, H', W']\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, N, C]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim=128, n_heads=4, depth=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(224, 16, emb_dim)\n",
    "        self.encoder = TransformerEncoder(emb_dim=emb_dim)\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.mlp_head(x[:, 0])  # CLS token\n",
    "\n",
    "class SiameseViT(nn.Module):\n",
    "    def __init__(self, emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = ViTEncoder(emb_dim=emb_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.encoder(x1)\n",
    "        out2 = self.encoder(x2)\n",
    "        return out1, out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dbdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartMatchDataset(Dataset):\n",
    "    def __init__(self, chart_dir, nonchart_dir, transform=None):\n",
    "        self.data = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for fname in os.listdir(chart_dir):\n",
    "            if fname.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.data.append((os.path.join(chart_dir, fname), 1))\n",
    "\n",
    "        for fname in os.listdir(nonchart_dir):\n",
    "            if fname.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.data.append((os.path.join(nonchart_dir, fname), 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.data[idx]\n",
    "        crop_img = Image.open(path).convert(\"RGB\")\n",
    "        page_img = Image.open(\"page.png\").convert(\"RGB\")  # Always use same page\n",
    "\n",
    "        if self.transform:\n",
    "            crop_img = self.transform(crop_img)\n",
    "            page_img = self.transform(page_img)\n",
    "\n",
    "        return crop_img, page_img, torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a49c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ChartMatchDataset(\"train/cropped\", \"train/nonchart\", transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = SiameseViT().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for img1, img2, labels in dataloader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1, out2 = model(img1, img2)\n",
    "        sim = torch.cosine_similarity(out1, out2)\n",
    "        loss = criterion(sim, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_crop(input_img, template_img):\n",
    "    ih, iw = input_img.shape[:2]\n",
    "    th, tw = template_img.shape[:2]\n",
    "\n",
    "    # If the template is too large, skip the matching\n",
    "    if th > ih or tw > iw:\n",
    "        print(f\"⚠️ Skipping template (too large): {template_img.shape} > {input_img.shape}\")\n",
    "        return None, -1\n",
    "\n",
    "    input_gray = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY)\n",
    "    template_gray = cv2.cvtColor(template_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Perform template matching\n",
    "    result = cv2.matchTemplate(input_gray, template_gray, cv2.TM_CCOEFF_NORMED)\n",
    "    _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "    # Get the position of the match (top-left corner)\n",
    "    h, w = template_gray.shape\n",
    "    top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "\n",
    "    # Crop the input image based on the match\n",
    "    cropped = input_img[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
    "    return cropped, max_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbba1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_and_predict(input_img, template_img, model, transform, device):\n",
    "    # Extract the cropped part that matches the chart\n",
    "    cropped, max_val = extract_crop(input_img, template_img)\n",
    "\n",
    "    if cropped is None:\n",
    "        return None, max_val\n",
    "\n",
    "    # Preprocess the cropped image for the model\n",
    "    cropped_img = Image.fromarray(cropped)\n",
    "    cropped_tensor = transform(cropped_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Use the Siamese ViT model to get the embeddings of the cropped image and the template\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out1, out2 = model(cropped_tensor, template_tensor)\n",
    "\n",
    "    # Compute similarity\n",
    "    sim = torch.cosine_similarity(out1, out2)\n",
    "\n",
    "    return sim, max_val\n",
    "\n",
    "# Example usage\n",
    "input_img = cv2.imread(\"page.png\")  # Example input image\n",
    "template_img = cv2.imread(r\"C:\\Users\\User_Guest\\Desktop\\mainak\\py\\train\\cropped\\cropped_2.png\")  # Example chart image (template)\n",
    "template_tensor = transform(template_img).unsqueeze(0).to(device)  # Convert to tensor\n",
    "\n",
    "\n",
    "sim, max_val = match_and_predict(input_img, template_img, model, transform, device)\n",
    "\n",
    "if sim is not None:\n",
    "    print(f\"Cosine Similarity: {sim.item()}, Match Quality: {max_val}\")\n",
    "else:\n",
    "    print(\"No match found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
